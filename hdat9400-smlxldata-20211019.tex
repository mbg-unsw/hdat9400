% HDAT9400 Data Management: S, M, L, XL Data
% (c) 2021 Malcolm Gillies <malcolm.gillies@unsw.edu.au>
% https://github.com/mbg-unsw/hdat9400
%
% This work is licensed under a
% Creative Commons Attribution-NonCommercial-ShareAlike 4.0
% International Licence
\documentclass[aspectratio=169,12pt,usepdftitle=false]{beamer} % XXXX fix AR here
\usepackage[latin1]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usefonttheme{serif} % need this with Charter font
\usetheme{Berlin}  % using default now
\usecolortheme{beaver}  % using default now
\usepackage[libertine]{libertine} % not using osf (old-style figures)
\usepackage[scale=0.9]{tgheros} % scale to match libertine
\usepackage[varqu,varl]{inconsolata}
\usepackage[libertine]{newtxmath}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{tikzpagenodes}
\usepackage[round]{natbib}
\usepackage{gitinfo2}

\hypersetup{pdfauthor={(C) 2021 Malcolm Gillies. CC-BY-NC-SA},
            pdftitle={HDAT9400 Data Management: S, M, L, XL Data},
	    pdfkeywords={data management, time complexity, data processing, health data, big data}}

\renewcommand{\gitMark}{\color{gray}\texttt{\tiny\gitBranch\,@\,\gitAbbrevHash\,\gitAuthorDate}}

\renewcommand{\bibsection}{} % suppress "References" section

\setbeamertemplate{navigation symbols}{} % remove navigation symbols
\setbeamertemplate{page number in head/foot}[totalpagenumber]
\setbeamercolor*{item}{fg=darkred}

\title{HDAT9400 Data Management: S, M, L, XL Data}
\institute{\url{https://github.com/mbg-unsw/hdat9400}}
\author{Malcolm Gillies}
\date{19 October 2021}
\usebackgroundtemplate{%
\begin{tikzpicture}[remember picture,overlay]
    \node[anchor=south west,scale=1,rotate=90] at ([shift={(0cm,0cm)}]current page marginpar area.south east) {\gitMark};
\end{tikzpicture}%
}

\titlegraphic{%
\begin{tikzpicture}[overlay,remember picture]
	\node[anchor=south west,scale=0.33] at ([shift={(0cm,0cm)}]current page text area.south west)
	{\includegraphics{ref/unsw-cbdrh-land}};
\end{tikzpicture}
}

\begin{document}

{
%\usebackgroundtemplate{}
\begin{frame}
\titlepage
\end{frame}
}

\begin{frame}{Lecture outline}
\end{frame}

\begin{frame}{About me}
    \begin{itemize}
        \item When I studied computer science (1990), a PC had
	    \begin{itemize}
		\item 4MB RAM (1000th today's phones)
		\item 200MB Disk (1000th today's phones)
		\item 33MHz Processor (100th today's phones)
	    \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Health data I've worked with}
    \begin{itemize}
	\item NPS MedicineWise: GP electronic medical records (MedicineInsight)
	\item NSW Ministry of Health: hospital, ambulance, births
	\item CBDRH: Pharmaceutical Benefits Scheme (PBS)
	\item SAS, R, MS SQL Server, PostgreSQL, SQLite, DuckDB
    \end{itemize}
\end{frame}

\begin{frame}{Why does data size matter?}
    \begin{itemize}
        \item Time and space are finite
	\item We have budgets and deadlines
	\item Two times bigger can take more than twice the time
    \end{itemize}
\end{frame}

% XXXX would be nice to have an example of a problem that becomes
% intractable at larger n

\begin{frame}{What can we do about it?}
    \begin{itemize}
	\item Work smarter, not harder
	\item Relax, people have been thinking about this for a long time!
    \end{itemize}
\end{frame}


% XXXX consider decimal alignment for Gigabytes column

\begin{frame}{How big are health data sets?}
    \begin{tabular}{lrr}
	\textbf{Data} & \textbf{Records} & \textbf{Gigabytes} \\
	NSW congenital conditions (5 years)& 10\,000 & 0.001 \\
	NSW perinatal (20 years) & 1\,000\,000 & 1 \\
	NSW Admitted patients (20 years) & 100\,000\,000 & 15 \\
	AU Pharmaceutical benefits (20 years) & 1\,000\,000\,000 & 400 \\
	XXXX Data Lake?? \\
    \end{tabular}\par
    \medskip
    20--200 variables per record
\end{frame}

\begin{frame}{Examples of different data processing technologies}
    \begin{tabular}{lrrl}
	\textbf{Method} & \textbf{Max size} & \textbf{Rec per sec} &
		\textbf{Notes} \\
	In memory [R] & xx & xx & Simple! \\
	Disk streaming [SAS] & 1TB & xx & Slower \\
	Relational database [PostgreSQL] & 1TB & xx & Complicated \\
	Column-store database [DuckDB] & 1TB & xx & Specialised \\
	NoSQL [Apache Spark] & ???? & ???? & Don't ask \\
    \end{tabular}
\end{frame}

\begin{frame}{Starting simple: process all the data}
    \begin{itemize}
	\item Sometimes you need to look at every record aka \emph{table scan}
	    \begin{itemize}
		\item e.g. What is the total length of stay of all NSW admissions?
	    \end{itemize}
	\item All else being equal, twice the data takes twice the time
	\item Most important distinction: scan in memory (RAM) or disk?
    \end{itemize}
\end{frame}

\begin{frame}{Making things more complicated}
    \begin{itemize}
	\item Sort all prescriptions by date of prescription
	\item Analyse all data from hospitals in Sydney
	\item For each antibiotic prescription, find the corresponding doctor visit
	\item Build a regression model for risk of low birth weight based on maternal characteristics
    \end{itemize}
\end{frame}

\begin{frame}{Experiment: sorting in SAS}
    \begin{itemize}
	\item XXXX
    \end{itemize}
\end{frame}

\begin{frame}{Time (and space) complexity}
    \begin{itemize}
	\item Asymptotic complexity % explain leading term etc
    \end{itemize}
\end{frame}

\begin{frame}{Speed of common algorithms}
    \begin{tabular}{ll}
	Sort & $\mathcal{O}(n\log{}n)$ \\
	(Binary) search & $\mathcal{O}(\log{}n)$ \\
	Matrix inversion & $\mathcal{O}(n^2\log{}n)$ \\ % XXXX
	XXXX % XXXX hashing, indexes...
    \end{tabular}
\end{frame}

\begin{frame}{Speeding up WHERE using an index}
    \begin{itemize}
	\item XXXX
    \end{itemize}
\end{frame}

\begin{frame}{Real world example: NSW hospital readmission rates I}
    \begin{itemize}
	\item Bureau of Health Information
	\item Quarterly report on hospital performance
	\item Mixed models, SAS
	\item Run time for the analysis: XXXX
    \end{itemize}
\end{frame}

\begin{frame}{Real world example: NSW hospital readmission rates II}
% insert some output fromt the report e.g. caterpillar plot, funnel plot
\centering
\includegraphics[height=0.75\textheight]
	{ref/ami-rsrr.pdf}

%	\tiny 9 March 2020

\end{frame}

\begin{frame}{Real world example: C****-19 cases daily reporting}
\end{frame}

\begin{frame}{Bonus round: what about \emph{big data}?}
    \begin{itemize}
	\item Parallel processing e.g. Google MapReduce
    \end{itemize}
\end{frame}

% -	What size data can you analyse with R, SAS, or an RDBMS?
%       Do some experiments and show results
% -	Digression on compiled v interpreted?
%
% -	Centre for Modestly Sized Data Research in Health
%
% -	Algorithms, efficiency and optimisation
% o	SQL and query optimisation
%
% -	Statistical modelling with bigger data
%
% -	Final real-world examples
% o	E.g. BHI hospital performance reporting, â€¦
%	**** ask BHI contact ****
%	Joe Hanna, Kha, Sadaf, Lilian
% o	Oisin, time-complexity of different GLMM approaches
% o	EHealth Data Lake
% o	Loading the HIE
%


% Breaks and diversions

% Points for discussion/questions

% -	Pop Quiz 1
% o	Which of these datasets is the biggest?

% -	Pop Quiz 2
% o	How long would it take to...

% -	Pop Quiz 3
% o	Which of these operations is fast? slow?
%	O(n log n) etc

% -	Pop Quiz 4
% o	Which of these cannot be computed within one lifetime?

% -	Potential diversion: SPDY

% -	Potential diversion: TP and Sabre

% Backgrounds: Rem Koolhaas buildings? (credits page)
\begin{frame}{Thanks}
    \begin{itemize}
        \item Sadaf Marashi-Pour (Bureau of Health Information)
	\item Sandy Sa (NSW Ministry of Health)
	\item Juan Quiroz Aguilera (CBDRH)
	\item Oisin Fitzgerald (CBDRH)
    \end{itemize}
\end{frame}

\begin{frame}{Further reading}
\end{frame}

\begin{frame}{References}
        \tiny\bibliography{hdat9400.bib}
        \bibliographystyle{abbrvnat}
\end{frame}

\end{document}
